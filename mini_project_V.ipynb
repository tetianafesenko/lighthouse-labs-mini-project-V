{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/tativalentine/Documents/GitHub/mini-project-V-1/train.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Shuffle the data to randomly sample data for testing\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Calculate the number of rows to use for testing\n",
    "test_size = int(train_data.shape[0] * 0.2)\n",
    "\n",
    "# Split the data into testing and training sets\n",
    "test_data = train_data.iloc[:test_size,:]\n",
    "train_data = train_data.iloc[test_size:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  323432\n",
      "Number of columns:  6\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the data\n",
    "print(\"Number of rows: \", train_data.shape[0])\n",
    "print(\"Number of columns: \", train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80858</th>\n",
       "      <td>2835</td>\n",
       "      <td>5624</td>\n",
       "      <td>5625</td>\n",
       "      <td>What are some cultural faux pas in space?</td>\n",
       "      <td>What are some cultural faux pas in China?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80859</th>\n",
       "      <td>44539</td>\n",
       "      <td>79917</td>\n",
       "      <td>79918</td>\n",
       "      <td>What is the fastest way to get rid of a canker...</td>\n",
       "      <td>What's the best way to get rid of a canker sore?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80860</th>\n",
       "      <td>352293</td>\n",
       "      <td>481213</td>\n",
       "      <td>481214</td>\n",
       "      <td>Can you explain global warming in layman terms?</td>\n",
       "      <td>What's the explanation of the connection betwe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80861</th>\n",
       "      <td>170408</td>\n",
       "      <td>263482</td>\n",
       "      <td>263483</td>\n",
       "      <td>How do I approach my parents about love?</td>\n",
       "      <td>How do I convince my parents for my love?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80862</th>\n",
       "      <td>362044</td>\n",
       "      <td>491931</td>\n",
       "      <td>491932</td>\n",
       "      <td>If the Star Trek Universe were real, could the...</td>\n",
       "      <td>Star Trek (creative franchise): Why are the br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    qid1    qid2  \\\n",
       "80858    2835    5624    5625   \n",
       "80859   44539   79917   79918   \n",
       "80860  352293  481213  481214   \n",
       "80861  170408  263482  263483   \n",
       "80862  362044  491931  491932   \n",
       "\n",
       "                                               question1  \\\n",
       "80858          What are some cultural faux pas in space?   \n",
       "80859  What is the fastest way to get rid of a canker...   \n",
       "80860    Can you explain global warming in layman terms?   \n",
       "80861           How do I approach my parents about love?   \n",
       "80862  If the Star Trek Universe were real, could the...   \n",
       "\n",
       "                                               question2  is_duplicate  \n",
       "80858          What are some cultural faux pas in China?             0  \n",
       "80859   What's the best way to get rid of a canker sore?             1  \n",
       "80860  What's the explanation of the connection betwe...             1  \n",
       "80861          How do I convince my parents for my love?             0  \n",
       "80862  Star Trek (creative franchise): Why are the br...             0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get a list of the column names\n",
    "print(\"Column names: \", train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows: \n",
      "            id    qid1    qid2  \\\n",
      "80858    2835    5624    5625   \n",
      "80859   44539   79917   79918   \n",
      "80860  352293  481213  481214   \n",
      "80861  170408  263482  263483   \n",
      "80862  362044  491931  491932   \n",
      "\n",
      "                                               question1  \\\n",
      "80858          What are some cultural faux pas in space?   \n",
      "80859  What is the fastest way to get rid of a canker...   \n",
      "80860    Can you explain global warming in layman terms?   \n",
      "80861           How do I approach my parents about love?   \n",
      "80862  If the Star Trek Universe were real, could the...   \n",
      "\n",
      "                                               question2  is_duplicate  \n",
      "80858          What are some cultural faux pas in China?             0  \n",
      "80859   What's the best way to get rid of a canker sore?             1  \n",
      "80860  What's the explanation of the connection betwe...             1  \n",
      "80861          How do I convince my parents for my love?             0  \n",
      "80862  Star Trek (creative franchise): Why are the br...             0  \n"
     ]
    }
   ],
   "source": [
    "# Get the first 5 rows of the data\n",
    "print(\"First 5 rows: \\n\", train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics: \n",
      "                   id           qid1           qid2   is_duplicate\n",
      "count  323432.000000  323432.000000  323432.000000  323432.000000\n",
      "mean   202004.845266  217092.798724  220718.269976       0.369926\n",
      "std    116661.324803  157631.435772  159790.502162       0.482785\n",
      "min         0.000000       1.000000       2.000000       0.000000\n",
      "25%    100998.750000   74426.750000   74605.000000       0.000000\n",
      "50%    201959.500000  191980.000000  196783.500000       0.000000\n",
      "75%    302937.750000  346253.250000  354212.500000       1.000000\n",
      "max    404288.000000  537930.000000  537931.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "# Get summary statistics for each column\n",
    "print(\"Summary statistics: \\n\", train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: \n",
      " id              0\n",
      "qid1            0\n",
      "qid2            0\n",
      "question1       1\n",
      "question2       2\n",
      "is_duplicate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values: \\n\", train_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the data\n",
    "\n",
    "df['question1'] = df['question1'].astype(str)\n",
    "df['question2'] = df['question2'].astype(str)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['question1'] = df['question1'].apply(preprocess_text)\n",
    "df['question2'] = df['question2'].apply(preprocess_text)\n",
    "\n",
    "# Convert text data into numerical representations\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df['question1'] + df['question2'])\n",
    "y = df['is_duplicate']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Preprocess both questions\n",
    "df['question1'] = df['question1'].apply(preprocess_text)\n",
    "df['question2'] = df['question2'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-idf\n",
    "tfidf = TfidfVectorizer()\n",
    "question1_tfidf = tfidf.fit_transform(df['question1'])\n",
    "question2_tfidf = tfidf.transform(df['question2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m question2_word2vec \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mquestion1\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     word_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mwv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m question\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mvocab]\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m word_vectors:\n\u001b[1;32m     11\u001b[0m         question1_word2vec\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(word_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m question2_word2vec \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mquestion1\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     word_vectors \u001b[39m=\u001b[39m [model\u001b[39m.\u001b[39mwv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m question\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mvocab]\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m word_vectors:\n\u001b[1;32m     11\u001b[0m         question1_word2vec\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(word_vectors, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "questions = df['question1'].tolist() + df['question2'].tolist()\n",
    "model = Word2Vec(questions, vector_size=150, window=10, min_count=2, workers=10)\n",
    "\n",
    "question1_word2vec = []\n",
    "question2_word2vec = []\n",
    "for question in df['question1']:\n",
    "    word_vectors = [model.wv[word] for word in question.split() if word in model.wv.vocab]\n",
    "    if word_vectors:\n",
    "        question1_word2vec.append(np.mean(word_vectors, axis=0))\n",
    "    else:\n",
    "        question1_word2vec.append(np.zeros(150))\n",
    "        \n",
    "for question in df['question2']:\n",
    "    word_vectors = [model.wv[word] for word in question.split() if word in model.wv.vocab]\n",
    "    if word_vectors:\n",
    "        question2_word2vec.append(np.mean(word_vectors, axis=0))\n",
    "    else:\n",
    "        question2_word2vec.append(np.zeros(150))\n",
    "        \n",
    "question1_word2vec = np.array(question1_word2vec)\n",
    "question2_word2vec = np.array(question2_word2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "question1_word_count = df['question1'].apply(lambda x: len(x.split()))\n",
    "question2_word_count = df['question2'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of the same words in both questions\n",
    "def same_words(question1, question2):\n",
    "    return len(set(question1.split()).intersection(question2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7625095847040491\n",
      "Precision:  0.723071863180398\n",
      "Recall:  0.5851938113458659\n",
      "F1 Score:  0.6468673568840912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['duplicate_question_classifier.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))\n",
    "\n",
    "# Save the best-performing model\n",
    "import joblib\n",
    "joblib.dump(clf, 'duplicate_question_classifier.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.84%\n"
     ]
    }
   ],
   "source": [
    "# Train the XGBoost classifier\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (acc * 100.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
